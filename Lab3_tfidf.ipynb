{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u\"======== 01-01\\nWritten by: Marta Kauffman & David Crane\\nMonica: There's nothing to tell! He's just some guy I work with!\\nJoey: C'mon, you're going out with the guy! There's gotta be something wrong wi\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents = unicode(open('data/friends_transcripts.txt').read(), errors='ignore')\n",
    "contents[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First element of ``re.split`` is empty string, so skip it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "666\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "splitted = re.split('======== (\\d\\d)-(\\d\\d)\\n', contents)[1:]\n",
    "print(len(splitted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import izip\n",
    "\n",
    "def tripletswise(t):\n",
    "    it = iter(t)\n",
    "    return izip(it,it,it)\n",
    "\n",
    "def clean(txt):\n",
    "    for i in string.punctuation:\n",
    "        txt = txt.replace(i, '')\n",
    "    return txt\n",
    "\n",
    "matched_data = [(int(x), int(y), clean(z)) for x, y, z in tripletswise(splitted)]\n",
    "N = len(matched_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first element is **season**,\n",
    "\n",
    "The second is **episode**,\n",
    "\n",
    "And the third â€“ actual **transcript**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def stem_data(data, use_stopwords=True):\n",
    "    def tokenize(doc):        \n",
    "        tokens = nltk.word_tokenize(doc.lower())\n",
    "        if use_stopwords:\n",
    "            return [t for t in tokens if not t in stopwords.words('english')]\n",
    "        else:\n",
    "            return tokens\n",
    "    \n",
    "    def stem(doc):\n",
    "        stemmer = PorterStemmer()\n",
    "        return [stemmer.stem(t) for t in tokenize(doc)]\n",
    "        \n",
    "    return [(seas, ep, stem(doc)) for seas, ep, doc in data]\n",
    "\n",
    "\n",
    "stemmed_data_w_stopwords = stem_data(matched_data, True)\n",
    "stemmed_data_wout_stopwords = stem_data(matched_data, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'written', u'marta', u'kauffman', u'david', u'crane', u'monica', u'there', u'noth', u'tell', u'he']\n",
      "[u'written', u'by', u'marta', u'kauffman', u'david', u'crane', u'monica', u'there', u'noth', u'to']\n"
     ]
    }
   ],
   "source": [
    "print(stemmed_data_w_stopwords[0][2][:10])\n",
    "print(stemmed_data_wout_stopwords[0][2][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def count_doc_frequencies(data):\n",
    "    doc_frequencies = Counter()\n",
    "    for _, _, doc in data:\n",
    "        for w in set(doc):\n",
    "            doc_frequencies[w] += 1\n",
    "    \n",
    "    return doc_frequencies\n",
    "\n",
    "\n",
    "docfreq_w_stopwords = count_doc_frequencies(stemmed_data_w_stopwords)\n",
    "docfreq_wout_stopwords = count_doc_frequencies(stemmed_data_wout_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'end', 216), (u'ross', 214), (u'well', 212), (u'dont', 212), (u'im', 211), (u'joey', 211), (u'oh', 211), (u'know', 210), (u'hey', 209), (u'your', 209)]\n",
      "[(u'by', 217), (u'end', 216), (u'you', 214), (u'ross', 214), (u'the', 213), (u'and', 213), (u'well', 212), (u'just', 212), (u'dont', 212), (u'are', 212)]\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "def sortdict(d):\n",
    "    return sorted(d.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    \n",
    "print(sortdict(docfreq_w_stopwords)[:10])\n",
    "print(sortdict(docfreq_wout_stopwords)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def calc_tfidf(word, freq, doc_freq):\n",
    "    tf = freq\n",
    "    idf = math.log(N / (doc_freq[word] + 1))\n",
    "    return tf * idf\n",
    "\n",
    "def calculate_tfidf(data, document_frequencies, limit=5):\n",
    "    result = []\n",
    "    for seas, ep, script in data:\n",
    "        metricised = {}\n",
    "        for word, count in Counter(script).iteritems():\n",
    "            metric = calc_tfidf(word, count, document_frequencies)\n",
    "            metricised[word] = metric\n",
    "\n",
    "        tfidf_weighted = sortdict(metricised)\n",
    "    \n",
    "        result.append((seas, ep, tfidf_weighted[:limit]))\n",
    "        \n",
    "    return result\n",
    "        \n",
    "tfidf_w_stopwords = calculate_tfidf(stemmed_data_w_stopwords, docfreq_w_stopwords)\n",
    "tfidf_wout_stopwords = calculate_tfidf(stemmed_data_wout_stopwords, docfreq_wout_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Season 1, episode 1: paul: 117.460, franni: 25.824, cut: 19.775, la: 15.390, goodnight: 12.712\n",
      "Season 1, episode 2: barri: 56.664, carol: 46.674, susan: 32.236, robbi: 28.257, marsha: 23.548\n",
      "Season 1, episode 3: alan: 87.036, lizzi: 48.088, paula: 28.257, smoke: 22.874, notnotmin: 18.838\n",
      "Season 1, episode 4: joann: 24.044, omnipot: 23.548, kiki: 23.548, receptionist: 21.666, pizza: 19.709\n",
      "Season 1, episode 5: angela: 64.997, bob: 39.586, janic: 33.271, laundri: 31.794, sud: 23.548\n"
     ]
    }
   ],
   "source": [
    "def print_tfidf_data(tfidf_data):\n",
    "    for seas, ep, metrics in tfidf_data:\n",
    "        freq = \", \".join(\"{}: {:6.3f}\".format(x, y) for x, y in metrics)\n",
    "        print(\"Season {}, episode {}: {}\".format(seas, ep, freq))\n",
    "        \n",
    "print_tfidf_data(tfidf_w_stopwords[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Season 1, episode 1: paul: 117.460, franni: 25.824, cut: 19.775, la: 15.390, goodnight: 12.712\n",
      "Season 1, episode 2: barri: 56.664, carol: 46.674, susan: 32.236, robbi: 28.257, marsha: 23.548\n",
      "Season 1, episode 3: alan: 87.036, lizzi: 48.088, paula: 28.257, smoke: 22.874, notnotmin: 18.838\n",
      "Season 1, episode 4: joann: 24.044, omnipot: 23.548, kiki: 23.548, receptionist: 21.666, pizza: 19.709\n",
      "Season 1, episode 5: angela: 64.997, bob: 39.586, janic: 33.271, laundri: 31.794, sud: 23.548\n"
     ]
    }
   ],
   "source": [
    "print_tfidf_data(tfidf_wout_stopwords[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 13, [(u'ronni', 122.44778523412069), (u'roger', 104.7166194666825), (u'tribbiani', 36.25420552604762), (u'boobi', 28.051332296627297), (u'mr', 22.873856958478193)])\n",
      "(1, 13, [(u'ronni', 122.44778523412069), (u'roger', 104.7166194666825), (u'tribbiani', 36.25420552604762), (u'ma', 34.43252074563336), (u'boobi', 28.051332296627297)])\n",
      "\n",
      "(2, 23, [(u'mandel', 4.30406509320417), (u'brown', 2.4849066497880004), (u'origin', 1.791759469228055), (u'written', 0.0), (u'end', 0.0)])\n",
      "(2, 23, [(u'mandel', 4.30406509320417), (u'brown', 2.4849066497880004), (u'origin', 1.791759469228055), (u'end', 0.0), (u'written', 0.0)])\n",
      "\n",
      "(7, 15, [(u'cecilia', 240.18604026692904), (u'jessica', 46.94193286437492), (u'dina', 36.06599866709224), (u'lockhart', 32.96671140918634), (u'own', 29.662531794038962)])\n",
      "(7, 15, [(u'cecilia', 240.18604026692904), (u'jessica', 46.94193286437492), (u'dina', 36.06599866709224), (u'lockhart', 32.96671140918634), (u'scottish', 25.824390559225023)])\n",
      "\n",
      "(8, 16, [(u'soul', 37.84189633918261), (u'mate', 34.33987204485146), (u'chees', 26.366694928034633), (u'waiter', 19.775021196025975), (u'bread', 9.534161491043838)])\n",
      "(8, 16, [(u'don', 76.13933051941696), (u'soul', 37.84189633918261), (u'mate', 34.33987204485146), (u'chees', 26.366694928034633), (u'waiter', 19.775021196025975)])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x, y in zip(tfidf_w_stopwords, tfidf_wout_stopwords):\n",
    "    if x != y:\n",
    "        print(x)\n",
    "        print(y)\n",
    "        print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good! Now just quickly do the same w/ stopwords for 2,3-grams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "bigrammed_data_w_stopwords = [(s, e, list(ngrams(d, 2))) for s, e, d in stemmed_data_w_stopwords]\n",
    "trigrammed_data_w_stopwords = [(s, e, list(ngrams(d, 3))) for s, e, d in stemmed_data_w_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Season 1, episode 1: (u'cut', u'cut'): 68.865, (u'wine', u'guy'): 23.548, (u'grab', u'spoon'): 18.838, (u'push', u'stair'): 18.838, (u'paul', u'wine'): 18.838\n",
      "Season 1, episode 2: (u'mr', u'geller'): 31.192, (u'ew', u'ew'): 28.051, (u'dr', u'oberman'): 17.216, (u'ross', u'marsha'): 14.129, (u'barri', u'yeah'): 12.022\n",
      "Season 1, episode 3: (u'feel', u'thing'): 14.444, (u'wouldnt', u'fair'): 14.129, (u'alan', u'alan'): 14.129, (u'footbal', u'phone'): 14.129, (u'alan', u'ross'): 14.129\n",
      "Season 1, episode 4: (u'pizza', u'guy'): 32.059, (u'she', u'walk'): 12.912, (u'im', u'okay'): 10.397, (u'kill', u'monica'):  9.534, (u'drop', u'towel'):  9.419\n",
      "Season 1, episode 5: (u'la', u'la'): 22.705, (u'bob', u'joey'): 14.129, (u'hey', u'hey'):  9.888, (u'actor', u'bob'):  9.419, (u'he', u'sophist'):  9.419\n"
     ]
    }
   ],
   "source": [
    "print_tfidf_data(calculate_tfidf(bigrammed_data_w_stopwords, count_doc_frequencies(bigrammed_data_w_stopwords))[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Season 1, episode 1: (u'cut', u'cut', u'cut'): 65.933, (u'paul', u'wine', u'guy'): 18.838, (u'shoe', u'your', u'shoe'): 12.912, (u'your', u'shoe', u'your'): 12.912, (u'im', u'la', u'vega'):  9.419\n",
      "Season 1, episode 2: (u'ew', u'ew', u'ew'): 24.044, (u'barri', u'yeah', u'well'):  9.419, (u'well', u'monica', u'ross'):  9.419, (u'gon', u'na', u'helen'):  9.419, (u'god', u'oh', u'god'):  8.959\n",
      "Season 1, episode 3: (u'thousand', u'dollar', u'footbal'):  9.419, (u'get', u'meet', u'guy'):  9.419, (u'seven', u'thousand', u'dollar'):  9.419, (u'realli', u'like', u'paula'):  9.419, (u'go', u'guy', u'friend'):  9.419\n",
      "Season 1, episode 4: (u'im', u'okay', u'okay'): 17.216, (u'pizza', u'guy', u'yeah'):  9.419, (u'okay', u'got', u'one'):  9.419, (u'see', u'lem', u'see'):  9.419, (u'she', u'walk', u'she'):  9.419\n",
      "Season 1, episode 5: (u'la', u'la', u'la'): 18.921, (u'hey', u'hey', u'hey'): 14.166, (u'real', u'job', u'go'):  9.419, (u'month', u'call', u'actor'):  9.419, (u'sophist', u'real', u'job'):  9.419\n"
     ]
    }
   ],
   "source": [
    "print_tfidf_data(calculate_tfidf(trigrammed_data_w_stopwords, count_doc_frequencies(trigrammed_data_w_stopwords))[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
